---
title: "HW 8"
author: "Matthew Litz"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages

library(tidyverse)
library(caret)
library(ISLR)
library(tree)
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**DS 6030 | Summer 2021 | University of Virginia **

*******************************************


# Question 8
In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.


(a) Split the data set into a training set and a test set.

```{r question_9a, echo=TRUE, warnings=FALSE, messages=FALSE}

carseats <- data.frame(Carseats)
attach(carseats)



#Create training and test splits
train.rows <- sample(rownames(carseats), dim(carseats)[1]*0.60)
carseats_train <- carseats[train.rows, ]
carseats_test <- carseats[setdiff(rownames(carseats), train.rows), ]
c(dim(carseats_train)[1], dim(carseats_test)[1])


```

(b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?



```{r question_8b, echo=TRUE}
set.seed(1)


#fit regression tree
tree.carseats <- rpart::rpart(Sales~., data=carseats_train, method='anova')
tree.carseats.party <- partykit::as.party(tree.carseats)
tree.carseats.party

#plot tree
ggparty::ggparty(tree.carseats.party, horizontal=TRUE) +
  ggparty::geom_edge() +
  ggparty::geom_edge_label() +
  ggparty::geom_node_label(aes(label=splitvar), ids="inner") + 
  ggparty::geom_node_plot(gglist = list(geom_histogram(aes(x=Sales), binwidth=3),
                                        theme(axis.title.x = element_blank(),
                                              axis.title.y = element_blank()))) +
  ggtitle('Decision Tree and resulting `Sales` bins')






```



(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?


```{r question_8c, echo=TRUE}

set.seed(1)
c <- rpart::rpart.control(minsplit=6,  minbucket=3, cp=0, maxdepth=9)
tree.v2.boston <- rpart::rpart(medv~., data=boston, method='anova', control=c)
tree.v2.boston.cp <- as_tibble(tree.v2.boston$cptable)$CP


set.seed(1)
boston.caret_rpart <- caret::train(medv~., data=boston, 
                                   method='rpart', 
                                   tuneGrid=data.frame(cp=tree.v2.boston.cp),
                                   control=c,
                      trControl=caret::trainControl("cv", number=5,
                                                    returnResamp='all'))






```





(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

```{r question_8d, echo=TRUE}




```





(e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the eï¬€ect of m, the number of variables considered at each split, on the error rate obtained.



```{r question_8e, echo=TRUE}




```





# Question 11
11. This question uses the Caravan data set.

```{r question_11, warning=FALSE, message=FALSE}

caravan <- data.frame(Caravan)
attach(caravan)


```


(a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.

```{r question_11a, warning=FALSE, message=FALSE}

#Create training and test splits
train.rows <- sample(rownames(caravan), dim(caravan)[1]*0.1718)
caravan_train <- caravan[train.rows, ]
caravan_test <- caravan[setdiff(rownames(caravan), train.rows), ]
c(dim(caravan_train)[1], dim(caravan_test)[1])


```


(b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?

```{r question_11b, echo=TRUE, warnings=FALSE, messages=FALSE}






```






(c) Use the boosting model to predict the response on the test data.  Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?


```{r question_11c, echo=TRUE, warnings=FALSE, messages=FALSE}







```



```

